---
title: "Evaluating Hash Function Distribution"
format: 
  revealjs:
    theme: [cosmo]
    transition: fade
    slide-number: true
    chalkboard: true
    code-line-numbers: true
    toc: true
    toc-depth: 2
jupyter: python3
---

## Introduction

- Hash functions map data to fixed-size integers
- Used in:
  - Hash tables
  - Sets
  - Bloom filters
- **Goal:** Distribute keys *uniformly* across buckets

---

## Desirable Properties of a Hash Function

- **Deterministic**: Same input always yields same output
- **Uniform**: Spreads input evenly over output range
- **Efficient**: Fast to compute
- **Low collision rate**: Different inputs produce different hashes

---

## Common Issues

- Clustering of values in few buckets
- Poor handling of similar inputs
- Non-uniform modulo distributions

---

## Built-in Python Hash

```python
print(hash("apple"))
print(hash("banana"))
print(hash("cherry"))
```

> Note: Python's `hash()` is salted by default â€” values vary per session

Use `PYTHONHASHSEED=0` to make results reproducible.

---

## Simulating Bucket Distribution

```python
import matplotlib.pyplot as plt
from collections import defaultdict

def bucket_distribution(data, num_buckets, hash_fn):
    buckets = defaultdict(list)
    for item in data:
        bucket = hash_fn(item) % num_buckets
        buckets[bucket].append(item)
    return buckets

data = [f"key{i}" for i in range(1000)]
num_buckets = 10
buckets = bucket_distribution(data, num_buckets, hash)

counts = [len(buckets[i]) for i in range(num_buckets)]

plt.bar(range(num_buckets), counts)
plt.xlabel("Bucket")
plt.ylabel("# of Keys")
plt.title("Bucket Distribution with Built-in hash()")
plt.show()
```

---

## Try With Poor Hash Function

```python
def poor_hash(key):
    return len(key)

buckets = bucket_distribution(data, num_buckets, poor_hash)
counts = [len(buckets[i]) for i in range(num_buckets)]

plt.bar(range(num_buckets), counts)
plt.title("Poor Hash Function: length of key")
plt.xlabel("Bucket")
plt.ylabel("# of Keys")
plt.show()
```

> This will likely result in **clustering** in a few buckets.

---

## Custom Hash Function Example

```python
def simple_ascii_sum(key):
    return sum(ord(char) for char in key)

buckets = bucket_distribution(data, num_buckets, simple_ascii_sum)
counts = [len(buckets[i]) for i in range(num_buckets)]

plt.bar(range(num_buckets), counts)
plt.title("Simple ASCII Sum Hash Function")
plt.xlabel("Bucket")
plt.ylabel("# of Keys")
plt.show()
```

> This function is better but still not great with similar inputs like `"key123"`.

---

## Visual Comparison

We can run multiple hash functions side by side and compare:

```python
hash_fns = {
    "Python hash()": hash,
    "ASCII Sum": simple_ascii_sum,
    "Length": poor_hash
}

fig, axs = plt.subplots(1, 3, figsize=(15, 4))

for ax, (label, fn) in zip(axs, hash_fns.items()):
    buckets = bucket_distribution(data, num_buckets, fn)
    counts = [len(buckets[i]) for i in range(num_buckets)]
    ax.bar(range(num_buckets), counts)
    ax.set_title(label)
    ax.set_xlabel("Bucket")

plt.suptitle("Hash Function Distribution Comparison")
plt.tight_layout()
plt.show()
```

---

## Measuring Distribution Quality

**Metrics to consider**:
- **Load Factor**: keys / buckets
- **Standard Deviation**: measures variance in bucket sizes
- **Max Bucket Size**: worst-case lookup time
- **Number of Collisions**: total items that shared a bucket

```python
import statistics

std_dev = statistics.stdev(counts)
max_bucket = max(counts)
collisions = sum(count - 1 for count in counts if count > 1)

print(f"Std Dev: {std_dev:.2f}")
print(f"Max Bucket Size: {max_bucket}")
print(f"Collisions: {collisions}")
```

---

## Best Practices

- Test with large and realistic datasets
- Avoid hashing based on length, subsets of key
- Mix character positions well
- Consider cryptographic hashes (`hashlib.md5`, `sha256`) if necessary

---

## When to Use Better Hashing

| Use Case           | Recommended Approach               |
|--------------------|------------------------------------|
| General dict/set   | Built-in `hash()` is fine          |
| Custom structures  | Use a good mixing function         |
| Security-critical  | Use `hashlib` or salted hashes     |
| Consistent hashing | Use libraries like `mmh3`, `xxhash`|

---

## Final Takeaways

- Uniform distribution is critical for **performance**
- Visualize your bucket usage!
- Understand how your hash function interacts with your key space
- Always test for **collisions** and **variance**

---

## Extra: Hashlib Example

```python
import hashlib

def hash_sha256(key):
    return int(hashlib.sha256(key.encode()).hexdigest(), 16)

buckets = bucket_distribution(data, num_buckets, hash_sha256)
counts = [len(buckets[i]) for i in range(num_buckets)]

plt.bar(range(num_buckets), counts)
plt.title("SHA-256 Hash Function")
plt.show()
```

---

## Questions?

ðŸ§  Feel free to experiment with your own datasets and hash functions!
